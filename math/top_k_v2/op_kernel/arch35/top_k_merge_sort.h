/**
* Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This program is free software, you can redistribute it and/or modify it under the terms and conditions of
 * CANN Open Software License Agreement Version 2.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */
/*!
 * \file top_k_merge_sort.h
 * \brief top_k_merge_sort entry
 */
#ifndef TOP_K_MERGE_SORT_H
#define TOP_K_MERGE_SORT_H
#include <cmath>
#include "kernel_operator.h"
#include "top_k_constant_var_simd.h"
#include "top_k_merge_sort_simd.h"
using namespace AscendC;
template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
struct MergeSort {
    __aicore__ inline MergeSort() {}
    __aicore__ inline void Init(
        GM_ADDR inputValue,
        GM_ADDR value,
        GM_ADDR indices,
        GM_ADDR workSpace,
        const TILING_DATA_TYPE* tilingData,
        TPipe* pipe);
    __aicore__ inline void ProcessSort();
    __aicore__ inline void ProcessSingleBlockSort(GlobalTensor<T> inputX);
    __aicore__ inline void CopyDataIn(
        GlobalTensor<T> inputX,
        uint64_t tileOffset,
        uint32_t currTileSize,
        uint32_t oneCoreRowNum);
    __aicore__ inline void CopyValue2Gm(
        uint64_t gmOffset,
        uint64_t tileOffset,
        uint32_t ouputLastDimValue,
        uint32_t oneCoreRowNum);
protected:
    __aicore__ inline void InitTilingData(const TILING_DATA_TYPE* tilingData);
    __aicore__ inline void InitBuffers(
        GM_ADDR inputValue,
        GM_ADDR value,
        GM_ADDR indices,
        GM_ADDR workSpace,
        TPipe* pipe);
    __aicore__ inline void InitVbs(TPipe* pipe);
public:
    TQue<QuePosition::VECIN, DOUBLE_BUFFER> inQueueX_;
    TQue<QuePosition::VECOUT, DOUBLE_BUFFER> outValueQueue_;
    TQue<QuePosition::VECOUT, DOUBLE_BUFFER> outIndexQueue_;

    // input value
    GlobalTensor<T> inputValueGm_;
    // output value
    GlobalTensor<T> outValueGm_;
    // output index
    GlobalTensor<INDEX_TYPE> outIndexGm_;

    uint32_t oneCoreRowNum_ = 0;
    uint32_t mergSortAcApiNeedBufferSize_ = 0;
    uint32_t numTileData_ = 0;
    uint32_t sortLoopRound_ = 0;
    uint32_t unsortedDimNum_ = 0;
    uint32_t unsortedDimParallel_ = 0;
    uint32_t sortLoopTimes_ = 0;
    uint32_t platformCoreNum_ = 0;
    uint32_t outputLastDimValue_ = 0;
    // merge sort kernel
    KernelVbsMergeSort<T, CONVERT_TYPE, IS_LARGEST> vbsSort;
};

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::Init(
    GM_ADDR inputValue,
    GM_ADDR value,
    GM_ADDR indices,
    GM_ADDR workSpace,
    const TILING_DATA_TYPE* tilingData,
    TPipe* pipe)
{
    InitTilingData(tilingData);
    InitBuffers(inputValue, value, indices, workSpace, pipe);
    // vbs init
    InitVbs(pipe);
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::InitTilingData(const TILING_DATA_TYPE* tilingData)
{
    // 尾轴size 512
    outputLastDimValue_ = tilingData->outputLastDimValue;
    numTileData_ = tilingData->numTileDataSize;
    unsortedDimNum_ = tilingData->unsortedDimNum;
    sortLoopTimes_ = tilingData->sortLoopTimes;
    unsortedDimParallel_ = tilingData->unsortedDimParallel;
    oneCoreRowNum_ = tilingData->oneCoreRowNum;
    // 高阶API需要的临时空间大小
    mergSortAcApiNeedBufferSize_ = tilingData->mergSortAcApiNeedBufferSize;
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::InitBuffers(GM_ADDR inputValue,
    GM_ADDR value, GM_ADDR indices, GM_ADDR workSpace, TPipe* pipe)
{
    inputValueGm_.SetGlobalBuffer((__gm__ T*)(inputValue));
    outValueGm_.SetGlobalBuffer((__gm__ T*)(value));
    outIndexGm_.SetGlobalBuffer((__gm__ INDEX_TYPE*)(indices));
    // init queue
    uint64_t realNum = ROUND_UP_AGLIN(numTileData_) * oneCoreRowNum_;
    pipe->InitBuffer(inQueueX_, DOUBLE_BUFFER, ROUND_UP_AGLIN(realNum) * sizeof(T));
    pipe->InitBuffer(outValueQueue_, DOUBLE_BUFFER, ROUND_UP_AGLIN(realNum * sizeof(T)));
    // 改为实际来支持int64_t
    pipe->InitBuffer(outIndexQueue_, DOUBLE_BUFFER, ROUND_UP_AGLIN(realNum * sizeof(INDEX_TYPE)));
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::InitVbs(TPipe* pipe)
{
    // vbs init
    vbsSort.SetPipe(pipe);
    vbsSort.MergeSortInitBuffer(numTileData_, oneCoreRowNum_, mergSortAcApiNeedBufferSize_);
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::ProcessSort()
{
    for(int32_t i = 0; i < sortLoopTimes_; i++) {
        sortLoopRound_ = i;
        uint64_t loopOffset = i * unsortedDimParallel_ * oneCoreRowNum_ * numTileData_;
        ProcessSingleBlockSort(inputValueGm_[loopOffset]);
    }
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::ProcessSingleBlockSort(GlobalTensor<T> inputX)
{
    uint32_t tileId = GetBlockIdx();
    uint32_t unsortedDimIndex = (GetBlockIdx() + sortLoopRound_ * unsortedDimParallel_) * oneCoreRowNum_;
    if (unsortedDimIndex >= unsortedDimNum_) {
        return;
    }
    uint32_t nowCoreRealRowNum = SortGetMin<uint32_t>((unsortedDimNum_ - unsortedDimIndex), oneCoreRowNum_);
    // get buffer
    AscendC::LocalTensor<T> sortedValueLocal = outValueQueue_.AllocTensor<T>();
    AscendC::LocalTensor<int64_t> sortedValueIndexInt64Local;
    AscendC::LocalTensor<uint32_t> sortedValueIndexLocal;
    if constexpr (is_same<int64_t, INDEX_TYPE>::value) {
        sortedValueIndexInt64Local = outIndexQueue_.AllocTensor<int64_t>();
        uint64_t realNum = ROUND_UP_AGLIN(numTileData_) * oneCoreRowNum_;
        sortedValueIndexLocal = sortedValueIndexInt64Local.template ReinterpretCast<uint32_t>()[realNum];
    } else {
        sortedValueIndexLocal = outIndexQueue_.AllocTensor<uint32_t>();
    }

    // offset
    uint64_t tileOffset = tileId * numTileData_ * oneCoreRowNum_;
    // copy data
    CopyDataIn(inputX, tileOffset, numTileData_, nowCoreRealRowNum);
    AscendC::LocalTensor<T> xLocal = inQueueX_.DeQue<T>();
    if constexpr (is_same<bfloat16_t, T>::value) {
        vbsSort.VbsMergeSortBf16(xLocal, sortedValueLocal, sortedValueIndexLocal,
                                 numTileData_, nowCoreRealRowNum);
    } else {
        vbsSort.VbsMergeSort(xLocal, sortedValueLocal, sortedValueIndexLocal,
                             numTileData_, nowCoreRealRowNum);
    }
    // 支持int64,此时把sort后的int32_t索引改为int64_t即可，里面的计算都用offset控制，输出再cast回去int32_t和int64_t即可
    if constexpr (is_same<int64_t, INDEX_TYPE>::value) {
        AscendC::LocalTensor<int32_t> sortedValueIndexInt32Local = sortedValueIndexLocal.template ReinterpretCast<int32_t>();
        AscendC::Cast(sortedValueIndexInt64Local, sortedValueIndexInt32Local,
            AscendC::RoundMode::CAST_NONE, nowCoreRealRowNum * ROUND_UP_AGLIN(numTileData_));
        outIndexQueue_.EnQue<int64_t>(sortedValueIndexInt64Local);
    } else {
        outIndexQueue_.EnQue<uint32_t>(sortedValueIndexLocal);
    }
    outValueQueue_.EnQue<T>(sortedValueLocal);
    inQueueX_.FreeTensor(xLocal);
    // copy result out
    uint64_t gmOffset = sortLoopRound_ * unsortedDimParallel_ * outputLastDimValue_ * oneCoreRowNum_;
    uint64_t answerTileOffset = tileId * outputLastDimValue_ * oneCoreRowNum_;

    CopyValue2Gm(gmOffset, answerTileOffset, outputLastDimValue_, nowCoreRealRowNum);
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::CopyDataIn(
    GlobalTensor<T> inputX,
    uint64_t tileOffset,
    uint32_t currTileSize,
    uint32_t oneCoreRowNum)
{
    LocalTensor<T> xLocal = inQueueX_.AllocTensor<T>();
    uint32_t aglinOneRowTileSize = ROUND_UP_AGLIN(currTileSize);
    uint32_t localTensorLen = aglinOneRowTileSize * oneCoreRowNum;
    T defaultValue = IS_LARGEST ? static_cast<T>(-INFINITY) : static_cast<T>(NAN);
    Duplicate(xLocal, defaultValue, localTensorLen);
    event_t eventId = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
    SetFlag<HardEvent::V_MTE2>(eventId);
    WaitFlag<HardEvent::V_MTE2>(eventId);
    uint32_t currTileSizeAlign = ROUND_UP_AGLIN(currTileSize * sizeof(T)) / sizeof(T);
    uint32_t dstStride =
        ((aglinOneRowTileSize - currTileSizeAlign) * sizeof(T)) / UB_AGLIN_VALUE;
    DataCopyPadExtParams<T> padParams;
    padParams.isPad = true;
    padParams.rightPadding = currTileSizeAlign - currTileSize;
    padParams.paddingValue = static_cast<T>(defaultValue);
    DataCopyExtParams dataCopyParam;
    dataCopyParam.blockCount = oneCoreRowNum;
    dataCopyParam.blockLen = currTileSize * sizeof(T);
    dataCopyParam.srcStride = 0;
    dataCopyParam.dstStride = dstStride;
    DataCopyPad(xLocal, inputX[tileOffset], dataCopyParam, padParams);
    inQueueX_.EnQue(xLocal);
}

template <typename T, typename CONVERT_TYPE, typename TILING_DATA_TYPE, bool IS_LARGEST, typename INDEX_TYPE>
__aicore__ inline void MergeSort<T, CONVERT_TYPE, TILING_DATA_TYPE, IS_LARGEST, INDEX_TYPE>::CopyValue2Gm(
    uint64_t gmOffset,
    uint64_t tileOffset,
    uint32_t outputLastDimValue,
    uint32_t oneCoreRowNum)
{
    uint32_t aglinOneRowTileSize = ROUND_UP_AGLIN(numTileData_);
    // value stride
    uint32_t currTileSizeAlign = ROUND_UP_AGLIN(outputLastDimValue * sizeof(T)) / sizeof(T);
    uint32_t ubStrideValue =
        ((aglinOneRowTileSize - currTileSizeAlign) * sizeof(T)) / UB_AGLIN_VALUE;
    // index stride
    uint32_t currSrcTileIndexSizeAlign =
        ROUND_UP_AGLIN(outputLastDimValue * sizeof(int32_t)) / sizeof(int32_t);
    uint32_t ubSrcStrideIndex =
        ((aglinOneRowTileSize - currSrcTileIndexSizeAlign) * sizeof(int32_t)) / UB_AGLIN_VALUE;

    uint32_t currTileIndexSizeAlign =
        ROUND_UP_AGLIN(outputLastDimValue * sizeof(INDEX_TYPE)) / sizeof(INDEX_TYPE);
    uint32_t ubStrideIndex =
        ((aglinOneRowTileSize - currTileIndexSizeAlign) * sizeof(INDEX_TYPE)) / UB_AGLIN_VALUE;
    // copy result out
    AscendC::LocalTensor<T> outValueLocal = outValueQueue_.DeQue<T>();
    AscendC::LocalTensor<INDEX_TYPE> outIndexLocal = outIndexQueue_.DeQue<INDEX_TYPE>();
    AscendC::DataCopyExtParams dataCopyParamValue{
        static_cast<uint16_t>(oneCoreRowNum),
        static_cast<uint32_t>(outputLastDimValue * sizeof(T)),
        ubStrideValue, 0, 0};
    AscendC::DataCopyPad(outValueGm_[gmOffset + tileOffset], outValueLocal, dataCopyParamValue);
    
    AscendC::DataCopyExtParams dataCopyParamIndex{
        static_cast<uint16_t>(oneCoreRowNum),   // 连续数据块的个数
        static_cast<uint32_t>(outputLastDimValue * sizeof(INDEX_TYPE)), // 每个连续传输数据块的长度，长度为Byte 
        ubStrideIndex,   // 源操作数，相邻连续数据块的间隔
        0, // 目的操作数，相邻连续数据块的间隔
        0}; 
    AscendC::DataCopyPad(outIndexGm_[gmOffset + tileOffset], outIndexLocal, dataCopyParamIndex);
    outIndexQueue_.FreeTensor(outIndexLocal);
    outValueQueue_.FreeTensor(outValueLocal);
}
#endif